{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397cef09-bd27-4769-9f70-7ad80803cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Nov 14 2021, 21:32:59) \n",
      "[Clang 12.0.5 (clang-1205.0.22.9)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae673f-f186-4e08-b7ee-23c236771a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_HOME = \"/Users/kun/github/spark/spark-3.1.2-bin-hadoop-3.2.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8491d5dd-2c63-4fd0-9bd6-cdeba9b970d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(SPARK_HOME)\n",
    "#findspark.add_packages([\"org.apache.hadoop:hadoop-aws:3.2.2\", \"com.amazonaws:aws-java-sdk-bundle:1.11.375\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167ea4d-2fb3-450c-82b2-c6362b454820",
   "metadata": {},
   "source": [
    "### Spark Session 생성\n",
    "\n",
    "로컬모드에서 실행할 Spark Session 을 만듭니다. (`.master(\"local[*]\")`)\n",
    "- 일반적인 Spark 설정은 `$SPARK_HOME/conf/spark-defaults.conf` 내에서 세팅해 공통환경으로 사용합니다. 다만 이 예제에서는 보여주기 위해 SparkConf 를 이용해 설정합니다.\n",
    "- Hive Metastore URI 등 HMS 관련 설정은 `$SPARK_HOME/conf/hive-site.conf` 내에서 세팅해 공통 환경으로 사용합니다.\n",
    "- 이 예제에서는 Minio 를 사용하므로 Access Key, Secret Key 를 사용합니다. AWS 위에서 실행된다면 [AWS Instance Profile](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html) 을 이용할 수 있으므로 키를 세팅하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27587697-2e5c-4301-bc98-82389915b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/29 15:41:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/29 15:41:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"example-app\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"accesskey\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"secretkey\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\",\"false\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.sparkContext.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52107cb8-9741-422e-b50a-d9cd830f5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c1bd63-5298-44fc-8681-974f2b9e7d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/26 01:44:27 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE student (\n",
    "    id INT, \n",
    "    name STRING, \n",
    "    age INT\n",
    ") \n",
    "STORED AS PARQUET\n",
    "LOCATION 's3a://udon-data/lake/student/'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f83e0e6-8337-4922-ab2f-9e13d7b7089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").load(\"s3a://udon-data-lake/marketing_campaign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccea55e-6ba0-4414-8588-e1968bddd92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
