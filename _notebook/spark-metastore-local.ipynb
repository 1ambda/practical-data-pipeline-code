{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "397cef09-bd27-4769-9f70-7ad80803cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Nov 14 2021, 21:32:59) \n",
      "[Clang 12.0.5 (clang-1205.0.22.9)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2499e6d-04da-4aa2-ac16-57917e975a47",
   "metadata": {},
   "source": [
    "### Spark 환경 설정\n",
    "\n",
    "로컬 머신 내 다운받은 SPARK_HOME 경로를 정확히 지정합니다. findspark() 를 통해 경로 내 Spark 를 실행할 수 있습니다.\n",
    "만약 AWS 환경이라면 EFS 등의 공유 스토리지에 Hadoop, Java, Spark 환경을 버전별로 구비해놓고 Read-only 로 마운트 해 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bae673f-f186-4e08-b7ee-23c236771a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/Users/kun/github/1ambda/practical-data-pipeline-code\"\n",
    "SPARK_HOME = \"/Users/kun/github/spark/spark-3.2.0-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8491d5dd-2c63-4fd0-9bd6-cdeba9b970d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init(SPARK_HOME)\n",
    "#findspark.add_packages([\"org.apache.hadoop:hadoop-aws:3.2.2\", \"com.amazonaws:aws-java-sdk-bundle:1.11.375\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167ea4d-2fb3-450c-82b2-c6362b454820",
   "metadata": {},
   "source": [
    "### Spark Session 생성\n",
    "\n",
    "로컬모드에서 실행할 Spark Session 을 만듭니다. (`.master(\"local[*]\")`)\n",
    "- 일반적인 Spark 설정은 `$SPARK_HOME/conf/spark-defaults.conf` 내에서 세팅해 공통환경으로 사용합니다. 다만 이 예제에서는 보여주기 위해 SparkConf 를 이용해 설정합니다.\n",
    "- Hive Metastore URI 등 HMS 관련 설정은 `$SPARK_HOME/conf/hive-site.conf` 내에서 세팅해 공통 환경으로 사용합니다.\n",
    "- 이 예제에서는 Minio 를 사용하므로 Access Key, Secret Key 를 사용합니다. AWS 위에서 실행된다면 [AWS Instance Profile](https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html) 을 이용할 수 있으므로 키를 세팅하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27587697-2e5c-4301-bc98-82389915b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/01 14:51:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/01 14:51:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"example-app\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbbee8-528f-4678-a535-66a65045e066",
   "metadata": {},
   "source": [
    "### Spark UI 확인\n",
    "\n",
    "http://localhost:4040 에서 Spark UI 를 확인할 수 있습니다.  \n",
    "만약 아래와 같은 메세지가 Spark Session 을 초기화 하는 과정에서 보였다면 Port 를 4041 (http://localhost:4041) 로 변경해 브라우저에서 확인합니다.\n",
    "\n",
    "```\n",
    "21/12/01 14:51:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "```\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "Production 환경에서는 사용자의 Jupyter Container (Kubernetes) 내에 https://github.com/jupyterhub/jupyter-server-proxy 등을 통해 제공할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc2125-5b8a-46ba-b4e1-5269d0a2f7aa",
   "metadata": {},
   "source": [
    "### Spark 설정 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52107cb8-9741-422e-b50a-d9cd830f5ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1638337901493'),\n",
       " ('spark.driver.port', '64469'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.0.2'),\n",
       " ('spark.app.startTime', '1638337900092'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/kun/github/1ambda/practical-data-pipeline-code/_notebook/spark-warehouse'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'example-app')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756b2b7-0dc0-49e1-9c7f-d7ca48813009",
   "metadata": {},
   "source": [
    "### 데이터 읽기\n",
    "\n",
    "Ecommerce 데이터셋 (약 2.4 GiB, 2천만 Rows) 를 읽습니다.\n",
    "만약 로컬 머신의 메모리가 부족하면 `_datasets/ecommerce` 파일 내 1 개의 CSV 만 남기고 나머지는 삭제해도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f83e0e6-8337-4922-ab2f-9e13d7b7089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(f\"{DATASET_ROOT}/_datasets/ecommerce/*.csv\", \n",
    "                     format=\"csv\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fe62979-29f5-4d58-bad3-279755848539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20692840"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabb637-b41b-4129-9efe-2e759744ee4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
