{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e86a47a-7483-47e1-9f60-de4b44061b55",
   "metadata": {},
   "source": [
    "## Kafka Library\n",
    "실행을 위해 다음 패키지 설치가 필요합니다.\n",
    "\n",
    "```\n",
    "pyenv activate pyspark \n",
    "\n",
    "pip install  kafka-python==2.0.2 msgpack==1.0.3 pandas==1.3.4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5101e36c-d1da-4020-9ffa-1dbf56082699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "from kafka.client_async import KafkaClient\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import TopicPartition\n",
    "\n",
    "import json\n",
    "import msgpack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfd1ea-e7e8-4283-a241-011f9a91efd4",
   "metadata": {},
   "source": [
    "### Kafka Broker 정보 확인\n",
    "\n",
    "Docker Compose 환경을 가정합니다. (로컬 환경 내 Kafka Broker 및 Zookeeper 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e750c7fd-53f4-4adb-bad4-34c2eb2c5f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "zookeeper_servers = [\"127.0.0.1:2181\"]\n",
    "bootstrap_servers = [\"127.0.0.1:9092\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d3a0c-6541-445b-a0fa-f6df8d640f4c",
   "metadata": {},
   "source": [
    "### Kafka Python API 사용\n",
    "\n",
    "[kafka-python](https://kafka-python.readthedocs.io/en/master/index.html) API 를 사용합니다.  \n",
    "[confluent-kafka-python](https://github.com/confluentinc/confluent-kafka-python) 을 사용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b451dd-a5ba-4553-a956-68754f80dba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{BrokerMetadata(nodeId='bootstrap-0', host='127.0.0.1', port=9092, rack=None)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = KafkaClient(bootstrap_servers=bootstrap_servers)\n",
    "client.cluster.brokers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f4f0fa-fe63-470b-af37-4911b10245b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = KafkaAdminClient(bootstrap_servers=bootstrap_servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fab865a-d198-4298-bf8d-b1f3e572b137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('schema-registry', 'sr'),\n",
       " ('user-event-relay-5c23dcf9-45d8-4493-9527-bc0316729aeb--409846158-driver-0',\n",
       "  'consumer')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin.list_consumer_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872bdf05-3d12-4cfa-b490-b8100dc388c2",
   "metadata": {},
   "source": [
    "### Topic 생성\n",
    "\n",
    "Kafka Partition 을 생성합니다. 일반적으로는 Kafka 관리자가 요청을 받아 UI 나 Console 에서 직접 생성할 수 있습니다.  \n",
    "API 를 이용해 생성하는 경우는 많지 않으나, 이 노트북에선 실습을 위해 API 를 통해 생성하는 예제를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e0139-e3aa-4a80-8411-27c4793aa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replication = 1, partition = 1 의 \"airbnb_listing_test\" 라는 토픽을 생성합니다.\n",
    "topic = \"user-event\"\n",
    "admin.create_topics([NewTopic(topic, 1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f31c624-396d-4e86-b3b9-c01a4cbcc300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user-event', '_schemas', '__consumer_offsets']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admin.list_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecb575-9e5c-451f-9fa5-3a3e17b84258",
   "metadata": {},
   "source": [
    "### Kafka Producer 를 이용해 데이터 전송\n",
    "\n",
    "kafka-python API 를 이용해 데이터를 Broker 로 전송합니다.  \n",
    "일반적으로는 Java / Scala 언어로 이루어진 개발한 Producer 나 Kafka Connect 등을 이용합니다.  \n",
    "\n",
    "다만 이 노트북에선 실습으로 Python API 를 이용하기 위해 Pandas 로 CSV 파일을 읽어 Python Kafka API 로 전송합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f7f91f-d3fc-4461-9fd9-c03e19ada426",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba0945a-4a70-4b7c-91bd-d1e2624778c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = \"/Users/kun/github/1ambda/practical-data-pipeline-code\"\n",
    "DATASET_AIRBNB_LISTING = f\"{DATASET_ROOT}/_datasets/ecommerce/2020-Feb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901c06ea-79e6-4008-bdbc-ad1ae07f6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.read_csv(DATASET_AIRBNB_LISTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "538434ac-1aad-4081-8d37-3567beaf9668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_time</th>\n",
       "      <th>event_type</th>\n",
       "      <th>product_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_code</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-01 00:00:01 UTC</td>\n",
       "      <td>cart</td>\n",
       "      <td>5844305</td>\n",
       "      <td>1487580006317032337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.14</td>\n",
       "      <td>485174092</td>\n",
       "      <td>4be9643a-420b-4c6b-83dd-a15e772fbf7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-01 00:00:03 UTC</td>\n",
       "      <td>view</td>\n",
       "      <td>5769925</td>\n",
       "      <td>1487580013841613016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kapous</td>\n",
       "      <td>4.22</td>\n",
       "      <td>594621622</td>\n",
       "      <td>a88baf11-9cd0-4362-bde4-1bfeed3f641d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                event_time event_type  product_id          category_id  \\\n",
       "0  2020-02-01 00:00:01 UTC       cart     5844305  1487580006317032337   \n",
       "1  2020-02-01 00:00:03 UTC       view     5769925  1487580013841613016   \n",
       "\n",
       "  category_code   brand  price    user_id  \\\n",
       "0           NaN     NaN   2.14  485174092   \n",
       "1           NaN  kapous   4.22  594621622   \n",
       "\n",
       "                           user_session  \n",
       "0  4be9643a-420b-4c6b-83dd-a15e772fbf7a  \n",
       "1  a88baf11-9cd0-4362-bde4-1bfeed3f641d  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "561ce6c6-871f-40b6-a204-a4e4dc4d79c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Row 는 Tuple 입니다. 이를 Dict 로 바꾸기 위해 to_dict 함수를 호출해\n",
    "# 전달 받은 Row (Dict) 를 Kafka Producer 를 이용해 Topic 으로 보냅니다.\n",
    "\n",
    "for row in pdf.head(10).to_dict(orient=\"records\"):\n",
    "    del row['category_id']\n",
    "    del row['category_code']\n",
    "    del row['brand']\n",
    "    message = json.dumps(row).encode('utf-8')\n",
    "\n",
    "    producer.send(topic, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cf3cc4-1739-4a4a-8a57-3cb816c25ff3",
   "metadata": {},
   "source": [
    "### Console Consumer 로 데이터 읽기\n",
    "\n",
    "이제 Broker 에 접근해 Console Consumer 로 해당 토픽 데이터를 읽어봅니다. 컨테이너에 들어가 다음 커맨드를 실행할 수 있습니다.  \n",
    "\n",
    "```\n",
    "kafka-console-consumer --bootstrap-server broker:9092 --topic airbnb_listing_test --from-beginning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b7053-8fb6-4ece-a610-28b2875dc80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
